{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "from geopy import distance\n",
    "\n",
    "raw_path = '/nfs/Projects/citibike/'\n",
    "data_path = './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the processed file already exists\n",
    "if os.path.exists('./citibike_original.h5'):\n",
    "    print('Found hdf file! Un-marshalling...')\n",
    "    \n",
    "    # just load it instead of reading from CSV again\n",
    "    df_whole = pd.read_hdf('./citibike_original.h5', key='df_whole')\n",
    "    \n",
    "else:\n",
    "    # read from .csv files\n",
    "    dtypes = {\n",
    "            'Unnamed: 0': np.int32, \n",
    "            'bikeid': np.int32,\n",
    "            'endstationlatitude': np.float64,\n",
    "            'endstationlongitude': np.float64,\n",
    "            'endstationname': 'str',\n",
    "            'gender': np.int16,\n",
    "            'startstationlatitude': np.float64,\n",
    "            'startstationlongitude': np.float64,\n",
    "            'startstationname': 'str',\n",
    "            'starttime': 'str',\n",
    "            'stoptime': 'str',\n",
    "            'tripduration': np.int32,\n",
    "            'usertype': 'str'\n",
    "        }\n",
    "    parse_dates = ['starttime', 'stoptime']\n",
    "\n",
    "    # loading the whole thing into memory\n",
    "    df_whole = pd.read_csv(raw_path + 'citibike.csv', dtype=dtypes, parse_dates=parse_dates)\n",
    "    df_whole.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "    # clean the data\n",
    "    # there are NaN in the startstationid and endstationid (the most important ones)\n",
    "    df_whole.dropna(subset=['startstationid', 'endstationid'], inplace=True)\n",
    "\n",
    "    df_whole = df_whole.astype({'startstationid': 'int32', 'endstationid':'int32'})\n",
    "    df_whole.drop_duplicates(subset=['bikeid', 'startstationid', 'endstationid', 'starttime', 'stoptime'], inplace=True)\n",
    "\n",
    "    # drop entries where starttime = endtime\n",
    "    df_whole.drop(df_whole[df_whole.starttime == df_whole.stoptime].index, inplace=True)\n",
    "\n",
    "    # Drop stations that have (lat, long) = (0, 0)\n",
    "    df_whole = df_whole[(df_whole['startstationlatitude'] != 0) & (df_whole['endstationlatitude'] != 0)]\n",
    "\n",
    "    # save the data to hdf5\n",
    "    df_whole.to_hdf('citibike_original.h5', key='df_whole', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We calculcate a dictionary to hold the key attributes of stations\n",
    "\n",
    "    stations_dict = {\n",
    "        stationid: {\n",
    "            index: (int)               // the index in tensors that each station id maps to\n",
    "            is_alive: (bool)           // whether the station is still alive\n",
    "                                          (dead stations will be excluded from prediction, but not from training)\n",
    "            earliest: (datetime)       // the time of the earliest entry involving stationid\n",
    "            latest: (datetime)         // the time of the latest entry involving stationid\n",
    "            alive_time: (int)          // the duration this station is alive (in hours)\n",
    "            \n",
    "            lat: (float)               // latitude\n",
    "            long: (float)              // longitude\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    \n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of the active stations from the official list\n",
    "# this does not include downloading from the web\n",
    "import json\n",
    "import os\n",
    "\n",
    "if os.path.exists(data_path + 'stations_dict.pickle'):\n",
    "    # load from disk\n",
    "    with (open(data_path + 'stations_dict.pickle', \"rb\")) as openfile:\n",
    "        stations_dict = pickle.load(openfile)\n",
    "\n",
    "else:\n",
    "    os.system('rm ./stations.json')\n",
    "    os.system('wget https://feeds.citibikenyc.com/stations/stations.json')\n",
    "\n",
    "    with open('stations.json', 'r') as jsonfile:\n",
    "        stations = json.load(jsonfile)['stationBeanList'] # stations contains the info for the stations\n",
    "\n",
    "    alive_station_set = set()\n",
    "    count = 0\n",
    "    for i, station in enumerate(sorted(stations, key = lambda i: i['id'])):\n",
    "        alive_station_set.add(station['id'])\n",
    "\n",
    "    # find the earliest and the lastest entries of each station\n",
    "    start_stations = df_whole.startstationid.unique()\n",
    "    end_stations = df_whole.endstationid.unique()\n",
    "    all_stations = np.union1d(start_stations, end_stations)\n",
    "\n",
    "    df_grouped_start = df_whole.sort_values('starttime').groupby('startstationid')\n",
    "    df_grouped_stop = df_whole.sort_values('stoptime').groupby('endstationid')\n",
    "\n",
    "    start_first = df_grouped_start.first()['starttime']\n",
    "    start_last = df_grouped_start.last()['starttime']\n",
    "    stop_first = df_grouped_stop.first()['stoptime']\n",
    "    stop_last = df_grouped_stop.last()['stoptime']\n",
    "\n",
    "    start_pos = df_grouped_start.last()[['startstationlatitude', 'startstationlongitude']].to_dict()\n",
    "    end_pos = df_grouped_stop.last()[['endstationlatitude', 'endstationlongitude']].to_dict()\n",
    "\n",
    "\n",
    "    # build a dictionary of the station data\n",
    "    stations_dict = {}\n",
    "    for i, stationid in enumerate(sorted(all_stations)):\n",
    "        info = {}\n",
    "\n",
    "        info['index'] = i\n",
    "        info['is_alive'] = stationid in alive_station_set\n",
    "\n",
    "        # time info\n",
    "        earliest_start = datetime.max\n",
    "        earliest_stop = datetime.max\n",
    "        latest_start = datetime.min\n",
    "        latest_stop = datetime.min\n",
    "\n",
    "        if stationid in start_first:\n",
    "            earliest_start = start_first[stationid]\n",
    "        if stationid in start_last:\n",
    "            latest_start = start_last[stationid]\n",
    "        if stationid in stop_first:\n",
    "            earliest_stop = stop_first[stationid]\n",
    "        if stationid in stop_last:\n",
    "            latest_stop = stop_last[stationid]\n",
    "\n",
    "        info['earliest'] = min(earliest_start, earliest_stop)\n",
    "        info['latest'] = max(latest_start, latest_stop)\n",
    "        info['alive_time'] = (info['latest'] - info['earliest']).total_seconds() / 3600\n",
    "        if info['alive_time'] <= 0:\n",
    "            info['alive_time'] = 0\n",
    "\n",
    "\n",
    "        # geo info\n",
    "        if stationid in start_pos['startstationlatitude']:\n",
    "            info['lat'] = start_pos['startstationlatitude'][stationid]\n",
    "            info['long'] = start_pos['startstationlongitude'][stationid]\n",
    "        elif stationid in end_pos['endstationlatitude']:\n",
    "            info['lat'] = end_pos['endstationlatitude'][stationid]\n",
    "            info['long'] = end_pos['endstationlongitude'][stationid]\n",
    "\n",
    "        stations_dict[stationid] = info\n",
    "\n",
    "    # save to disk\n",
    "    with open(data_path + 'stations_dict.pickle', 'wb') as handle:\n",
    "        pickle.dump(stations_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Station-Station Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_distance = {} # pair wise distance using geopy.distance calculations\n",
    "for startid in stations_dict.keys():\n",
    "    \n",
    "    origin = (stations_dict[startid]['lat'], stations_dict[startid]['long'])\n",
    "    \n",
    "    dist_temp = {}\n",
    "    for endid in stations_dict.keys():\n",
    "        \n",
    "        destination = (stations_dict[endid]['lat'], stations_dict[endid]['long'])\n",
    "        dist_temp[endid] = distance.distance(origin, destination).m\n",
    "        \n",
    "    station_distance[startid] = dist_temp\n",
    "    \n",
    "with open(data_path + 'station_distance.pickle', 'wb') as handle:\n",
    "    pickle.dump(station_distance, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Station-Station Average Ride Count (per day)\n",
    "\n",
    "ride_count is a dictionary of dictionary that holds the average ride count for all stations\n",
    "\n",
    "    ride_count = {\n",
    "       startstationid: {\n",
    "           'endstationid_1': count\n",
    "           'endstationid_2': count\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count = df_whole.groupby(['startstationid', 'endstationid']).count()['bikeid']\n",
    "\n",
    "ride_count = {}\n",
    "ride_accumulate = {}\n",
    "for startid in stations_dict.keys():\n",
    "    dct = {}\n",
    "    for endid in stations_dict.keys(): \n",
    "        # count\n",
    "        try:\n",
    "            count = df_count[startid, endid]\n",
    "        except:\n",
    "            count = 0\n",
    "            \n",
    "        # interval\n",
    "        if count == 0:\n",
    "            dct[endid] = 0\n",
    "        else:\n",
    "            # note that we are using source station as the time reference\n",
    "            if stations_dict[startid]['alive_time'] == 0:\n",
    "                dct[endid] = 0\n",
    "            else:\n",
    "                dct[endid] = count / stations_dict[startid]['alive_time']\n",
    "                \n",
    "    ride_count[startid] = dct\n",
    "    \n",
    "with open(data_path + 'ride_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(ride_count, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Station-Station Per-Hour Inbound/Outbound Correlation\n",
    "\n",
    "there are two dictionaries for this\n",
    "\n",
    "    in_hist = {\n",
    "        'endstationid': {\n",
    "            * influx from 00:00 to 01:00 * (if exists), \n",
    "            * influx from 01:00 to 02:00 *,\n",
    "            ...\n",
    "            * influx from 23:00 to 24:00 *\n",
    "        }\n",
    "     }\n",
    " \n",
    "    out_hist = {\n",
    "        'startstationid': {\n",
    "            * inflow from 00:00 to 01:00 *,\n",
    "            * inflow from 01:00 to 02:00 *,\n",
    "            ...\n",
    "            * inflow from 23:00 to 24:00 *\n",
    "        }\n",
    "    }\n",
    "\n",
    "The values are the count of all the data from 2013-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.6 s, sys: 7.65 s, total: 35.3 s\n",
      "Wall time: 35.3 s\n"
     ]
    }
   ],
   "source": [
    "df_whole_endindex = df_whole.set_index('stoptime')\n",
    "df_group_end = df_whole_endindex.groupby(['endstationid', df_whole_endindex.index.hour]).count()['bikeid']\n",
    "\n",
    "in_hist_count = {int(stationid): df_group_end.xs(stationid).to_dict() for stationid in df_group_end.index.levels[0]}\n",
    "\n",
    "# divide the count by alive_time to get the average per hour\n",
    "in_hist = {}\n",
    "for endid in stations_dict.keys():\n",
    "    if endid in in_hist_count:\n",
    "        alive_time = stations_dict[endid]['alive_time']\n",
    "        \n",
    "        if alive_time != 0:\n",
    "            in_hist[endid] = {v / alive_time for k, v in in_hist_count[endid].items()}\n",
    "            \n",
    "            \n",
    "df_whole_startindex = df_whole.set_index('starttime')\n",
    "df_group_start = df_whole_startindex.groupby(['startstationid', df_whole_startindex.index.hour]).count()['bikeid']\n",
    "\n",
    "out_hist_count = {int(stationid): df_group_start.xs(stationid).to_dict() for stationid in df_group_start.index.levels[0]}\n",
    "\n",
    "out_hist = {}\n",
    "for startid in stations_dict.keys():\n",
    "    if startid in out_hist_count:\n",
    "        alive_time = stations_dict[startid]['alive_time']\n",
    "        \n",
    "        if alive_time != 0:\n",
    "            out_hist[endid] = {v / alive_time for k, v in out_hist_count[startid].items()}\n",
    "\n",
    "with open(data_path + 'in_hist.pickle', 'wb') as handle:\n",
    "    pickle.dump(in_hist, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "with open(data_path + 'out_hist.pickle', 'wb') as handle:\n",
    "    pickle.dump(out_hist, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outflow\n",
    "df_whole_startindex = df_whole.set_index('starttime')\n",
    "df_out_sample_h = df_whole_startindex.groupby('startstationid').resample('H').count()['bikeid']\n",
    "\n",
    "df_whole_endindex = df_whole.set_index('stoptime')\n",
    "df_in_sample_h = df_whole_endindex.groupby('endstationid').resample('H').count()['bikeid']\n",
    "\n",
    "time_range = pd.date_range(start='2013-06-01', end='2019-03-01', freq='H')[:-1]\n",
    "\n",
    "# data is the dataframe that hold all the to-be training data\n",
    "frame = {'timestamp': time_range, 'inflow': None, 'outflow': None}\n",
    "data = pd.DataFrame(frame).set_index('timestamp')\n",
    "\n",
    "num_stations = len(stations_dict)\n",
    "for time_stamp in time_range:\n",
    "    df_inflows = df_in_sample_h[:, time_stamp]\n",
    "    df_outflows = df_out_sample_h[:, time_stamp]\n",
    "    \n",
    "    inflow = np.zeros(num_stations)\n",
    "    outflow = np.zeros(num_stations)\n",
    "\n",
    "    # populate inflow\n",
    "    for i, val in df_inflows.items():\n",
    "        inflow[stations_dict[i]['index']] = val\n",
    "    \n",
    "    for i, val in df_outflows.items():\n",
    "        outflow[stations_dict[i]['index']] = val\n",
    "        \n",
    "    data.at[time_stamp] = inflow, outflow\n",
    "    \n",
    "with open(data_path + 'data.pickle', 'wb') as handle:\n",
    "    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data into huge `Data` lists.\n",
    "if os.path.exists(data_path + 'train_data_list.pickle'):\n",
    "    train_data_list = pickle.load(openfile)\n",
    "    print(len(train_data_list))\n",
    "    \n",
    "else:\n",
    "    # load in preprocessed inflow-outflow data\n",
    "    with open(data_path + 'data.pickle', 'rb') as openfile:\n",
    "        df_data = pickle.load(openfile)\n",
    "\n",
    "    # read in the weather data\n",
    "    with open(data_path + 'weather.pickle', 'rb') as openfile:\n",
    "        df_weather = pickle.load(openfile)\n",
    "        \n",
    "    df_weather.drop(columns=['index'], inplace=True)\n",
    "    \n",
    "    # get the time stamp of the split\n",
    "    break_point = df_data.index[-1] - pd.Timedelta(days=40)\n",
    "\n",
    "    df_train_data = df_data[df_data.index < break_point]\n",
    "    df_test_data = df_data[df_data.index >= break_point]\n",
    "    \n",
    "    # Train data\n",
    "    train_data_list = []\n",
    "    for i in range(len(df_train_data) - 1):\n",
    "        feat = np.array([df_train_data.iloc[i]['inflow'], df_train_data.iloc[i]['outflow']]).T\n",
    "        target = np.array([df_train_data.iloc[i + 1]['inflow'], df_train_data.iloc[i + 1]['outflow']]).T\n",
    "\n",
    "        # this gets the time difference between the data[i] and the first entry in df_weather\n",
    "        # this can be used as an index to retrieve the weather data for the prediction target\n",
    "        index_in_weather = (df_train_data.index[i + 1] - df_train_data.index[0]).days\n",
    "        weather_data = np.array(df_weather.iloc[index_in_weather].values)\n",
    "\n",
    "        x = torch.FloatTensor(feat)\n",
    "        y = torch.FloatTensor(target)\n",
    "        target_weather = torch.FloatTensor(weather_data)\n",
    "\n",
    "        train_data_list.append(Data(x=x, y=y, edge_index=edge_index, edge_weight=edge_weight, target_weather=target_weather))\n",
    "\n",
    "    with open(data_path + 'train_data_list.pickle', 'wb') as handle:\n",
    "        pickle.dump(train_data_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "    # Test data\n",
    "    test_data_list = []\n",
    "    for i in range(len(df_test_data) - 1):\n",
    "        feat = np.array([df_test_data.iloc[i]['inflow'], df_test_data.iloc[i]['outflow']]).T\n",
    "        target = np.array([df_test_data.iloc[i + 1]['inflow'], df_test_data.iloc[i + 1]['outflow']]).T\n",
    "\n",
    "        index_in_weather = (df_test_data.index[i + 1] - df_test_data.index[0]).days\n",
    "        weather_data = np.array(df_weather.iloc[index_in_weather].values)\n",
    "\n",
    "        x = torch.FloatTensor(feat)\n",
    "        y = torch.FloatTensor(target)\n",
    "        target_weather = torch.FloatTensor(weather_data)\n",
    "\n",
    "        test_data_list.append(Data(x=x, y=y, edge_index=edge_index, edge_weight=edge_weight, target_weather=target_weather))\n",
    "\n",
    "    with open(data_path + 'test_data_list.pickle', 'wb') as handle:\n",
    "        pickle.dump(test_data_list, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather = pd.read_csv('weather.csv')\n",
    "df_weather = df_weather.loc[df_weather['REPORT_TYPE'] == 'SOD  ']\n",
    "\n",
    "keep_column_list = [\n",
    "    'DATE',\n",
    "    'DailyAverageDryBulbTemperature',\n",
    "    'DailyAverageRelativeHumidity',\n",
    "    'DailyAverageStationPressure',\n",
    "    'DailyCoolingDegreeDays',\n",
    "    'DailyDepartureFromNormalAverageTemperature',\n",
    "    'DailyHeatingDegreeDays',\n",
    "    'DailyMaximumDryBulbTemperature',\n",
    "    'DailyMinimumDryBulbTemperature',\n",
    "    'DailyPrecipitation',\n",
    "    'DailySnowDepth',\n",
    "    'DailySnowfall'\n",
    "]\n",
    "df_weather = df_weather[keep_column_list]\n",
    "df_weather = df_weather.replace({'T': 0.0})\n",
    "df_weather = df_weather.astype({'DATE': 'str', 'DailyPrecipitation': 'float64', 'DailySnowDepth': 'float64', 'DailySnowfall': 'float64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing date:  2013-12-31 23:59:00\n",
      "missing date:  2014-12-31 23:59:00\n"
     ]
    }
   ],
   "source": [
    "norm_columns = [\n",
    "    'DailyAverageDryBulbTemperature',\n",
    "    'DailyAverageRelativeHumidity',\n",
    "    'DailyAverageStationPressure',\n",
    "    'DailyCoolingDegreeDays',\n",
    "    'DailyDepartureFromNormalAverageTemperature',\n",
    "    'DailyHeatingDegreeDays',\n",
    "    'DailyMaximumDryBulbTemperature',\n",
    "    'DailyMinimumDryBulbTemperature',\n",
    "    'DailyPrecipitation',\n",
    "    'DailySnowDepth',\n",
    "    'DailySnowfall'\n",
    "]\n",
    "\n",
    "normalized_df_weather = df_weather\n",
    "\n",
    "# normalizing\n",
    "normalized_df_weather[norm_columns] = (df_weather[norm_columns] - df_weather[norm_columns].mean()) / df_weather[norm_columns].std()\n",
    "\n",
    "# There are missing dates in the weather data (2013-12-31)\n",
    "# We get the dummy data from interpolating\n",
    "normalized_df_weather = normalized_df_weather.reset_index(drop=True)\n",
    "normalized_df_weather['DATE'] = pd.to_datetime(normalized_df_weather['DATE'])\n",
    "normalized_df_weather.set_index('DATE', inplace=True)\n",
    "\n",
    "counter = 0\n",
    "for i in normalized_df_weather.index:\n",
    "    if counter != (i - pd.Timestamp('2013-06-01 00:00:00')).days:\n",
    "        # insert the missing date with NaN values (for future interpolation)\n",
    "        ts = pd.Timestamp('2013-06-01 23:59:00') + pd.Timedelta(days=counter)\n",
    "        \n",
    "        print('missing date: ', ts)\n",
    "        \n",
    "        new_row = pd.DataFrame(index=[ts]) \n",
    "        normalized_df_weather = pd.concat([normalized_df_weather, new_row], ignore_index=False, sort=False)\n",
    "        \n",
    "        counter = (i - pd.Timestamp('2013-06-01 00:00:00')).days\n",
    "        \n",
    "    counter += 1\n",
    "    \n",
    "normalized_df_weather.sort_index(inplace=True)\n",
    "normalized_df_weather = normalized_df_weather.interpolate()\n",
    "\n",
    "# turn day of week into one-hot encoding\n",
    "normalized_df_weather = normalized_df_weather.reset_index()\n",
    "day_name = normalized_df_weather['index'].dt.day_name()\n",
    "df_dayname_dummies = pd.get_dummies(pd.Categorical(day_name), prefix='DoW_')\n",
    "normalized_df_weather = pd.concat([normalized_df_weather, df_dayname_dummies], axis=1)\n",
    "\n",
    "normalized_df_weather = normalized_df_weather.fillna(0)\n",
    "\n",
    "with open(data_path + 'weather.pickle', 'wb') as handle:\n",
    "    pickle.dump(normalized_df_weather, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tops = df_whole.groupby(['startstationid', 'endstationid']).count().sort_values(['bikeid'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_stations = set()\n",
    "\n",
    "for index in df_tops.index:\n",
    "    top_stations.add(index[0])\n",
    "    top_stations.add(index[1])\n",
    "    \n",
    "    if len(top_stations) >= 10:\n",
    "        break\n",
    "\n",
    "top_station_index = []\n",
    "# convert stationid into node index\n",
    "for stationid in top_stations:\n",
    "    top_station_index.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c4ba565118c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'stations_dict.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopenfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mstations_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopenfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "with open(data_path + 'stations_dict.pickle', 'rb') as openfile:\n",
    "    stations_dict = pickle.load(openfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
